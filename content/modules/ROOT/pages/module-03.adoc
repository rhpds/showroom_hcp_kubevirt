= Creating a Hosted Cluster on OpenShift Virtualization

You can use the hosted control plane command line interface, `hcp``, to create an *OpenShift Container Platform hosted cluster*. The hosted cluster is automatically imported as a managed cluster.

== Prerequisites

. Connect to the bastion host
+
[source,bash]
----
[lab-user@hypervisor ~]$ sudo ssh root@192.168.123.100
----


. Login to OCP
+
[source,bash,subs="attributes"]
----
[root@ocp4-bastion ~]# oc login {ocp_api} --username={ocp_username} --password={ocp_password}
----

. Allow wildcard routes
+
[source,bash,subs="attributes"]
----
[root@ocp4-bastion ~]# oc patch ingresscontroller -n openshift-ingress-operator default --type=json -p '[{ "op": "add", "path": "/spec/routeAdmission", "value": {wildcardPolicy: "WildcardsAllowed"}}]'
----



== Install CLI





. Download the cli called `hcp`
+
[source,bash,subs="attributes"]
----
[root@ocp4-bastion ~]# curl -O https://hcp-cli-download-multicluster-engine.apps.{guid}.dynamic.redhatworkshops.io/linux/amd64/hcp.tar.gz
----

. Extract the client
+
[source,bash]
----
[root@ocp4-bastion ~]# tar xvfz hcp.tar.gz -C /usr/local/bin/
----

. Confirm the command `hcp` is available 
+
[source,bash]
----
[root@ocp4-bastion ~]# hcp -v
----
+
.Sample Output
+
[%nowrap]
----
hcp version openshift/hypershift: e87182ca75da37c74b371aa0f17aeaa41437561a. Latest supported OCP: 4.14.0
----


[#create]
== Create Hosted Cluster

Some considerations:

* Run the hub cluster and workers on the same platform for hosted control planes.
* Each hosted cluster must have a unique name in order for multicluster engine operator to manage it.
* A hosted cluster cannot be created in the namespace of a multicluster engine operator managed cluster.

. Create a new hosted cluster named _cluster1_ using the command `hcp`:
+
[source,bash]
----
[root@ocp4-bastion ~]# hcp create cluster kubevirt \
--name cluster1 \
--release-image quay.io/openshift-release-dev/ocp-release:4.14.1-x86_64 \
--node-pool-replicas 2 \
--pull-secret ~/pull-secret.json \
--memory 6Gi \
--cores 2
----
+
.Sample Output
+
[%nowrap]
----
2023-12-02T21:49:55Z    INFO    Applied Kube resource   {"kind": "Namespace", "namespace": "", "name": "clusters"}
2023-12-02T21:49:55Z    INFO    Applied Kube resource   {"kind": "Secret", "namespace": "clusters", "name": "cluster1-pull-secret"}
2023-12-02T21:49:55Z    INFO    Applied Kube resource   {"kind": "", "namespace": "clusters", "name": "cluster1"}
2023-12-02T21:49:55Z    INFO    Applied Kube resource   {"kind": "Secret", "namespace": "clusters", "name": "cluster1-etcd-encryption-key"}
2023-12-02T21:49:55Z    INFO    Applied Kube resource   {"kind": "NodePool", "namespace": "clusters", "name": "cluster1"}
----
+
[NOTE]
You can use the `--release-image`` flag to set up the hosted cluster with a specific OpenShift Container Platform release.
+
A default node pool is created for the cluster with two virtual machine worker replicas according to the `--node-pool-replicas` flag.

. Verify the host control plane pods running inside of the new namespace created (`clusters-cluster1`)
+
[%nowrap]
----
[root@ocp4-bastion ~]# oc get pod -n clusters-cluster1
----
+
.Sample Output
+
[%nowrap]
----
NAME                                      READY   STATUS    RESTARTS   AGE
capi-provider-554c58b965-6cx78            1/1     Running   0          4m23s
cluster-api-5f5b78d889-2tnqm              1/1     Running   0          4m24s
control-plane-operator-67b7d4556b-4b4mq   1/1     Running   0          4m23s
----

. Check the status of the host clusters, querying the _Custom Resource_ named `HostedCluster`. 
+
[%nowrap]
----
[root@ocp4-bastion ~]# oc get --namespace clusters hostedclusters
----
+
.Sample Output
+
[%nowrap]
----
NAME       VERSION   KUBECONFIG                  PROGRESS   AVAILABLE   PROGRESSING   MESSAGE
cluster1             cluster1-admin-kubeconfig   Partial    False       False         Waiting for Kube APIServer deployment to become available
----
+
[IMPORTANT]
It takes some minutes till the cluster switches from Partial to Completed. Don't wait.


. Go back to the ACM Console (`All Clusters` in the top menu)
. Notice the `cluster1` will appear automatically
+
image::_images/Install/01_ACM_Review.png[]

. Click on `cluster1` and check the progress
+
image::_images/Install/02_ACM_Progress.png[]

. Review the information and scroll down and wait till the status is `Pending import`.

. Meantime the installation continues, check OpenShift Virtualization
.. Select `local-cluster` in the top menu
.. In the left menu navigate to *Virtualization* -> *Virtual Machines*
.. Select project `clusters-cluster1`
+
image::_images/Install/03_OCPV_VMs.png[]
+
CoreOS disks are imported automatically and the VMs starts. They will act as a workers for the hosted control planes cluster.


. In the left menu navigate to *Networking* -> *Services*:
+
image::_images/Install/04_OCPV_Services.png[]
+
Notice the required services for a OpenShift are created inside the namespace for each cluster created.

. Navigate to *Networking* -> *Services*:
+
image::_images/Install/05_OCPV_Routes.png[]
+
The routes to access from outside to the hosted cluster are listed.

. Navigate to *Storage* -> *PersistentVolumeClaims*
+
image::_images/Install/06_OCPV_PVCs.png[]
+
Notice the etcd disk for the control planes are created. This disk are used on the control planes pods. It is recommended to use a low-latency and fast I/O disks for etcd to avoid issues.

. Go back to *ACM* console and select `cluster1` and wait till the creation finishes.
+
image::_images/Install/07_OCPV_Ready.png[]
+
The cluster can be `Ready` but the workers still provisioning. Wait till the `Cluster node pools` section moves from `Pending` to `Ready`



TODO:

Go to ACM, reveal credentials, connect to cluster


hcp create kubeconfig --name cluster1 > cluster1-kubeconfig

oc get co --kubeconfig=cluster1-kubeconfig



Review cluster