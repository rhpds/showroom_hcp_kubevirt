= Creating a Hosted Cluster on OpenShift Virtualization

You can use the hosted control plane command line interface, `hcp``, to create an *OpenShift Container Platform hosted cluster*. The hosted cluster is automatically imported as a managed cluster.

== Install CLI

. Connect to the bastion host
+
[source,bash]
----
[lab-user@hypervisor ~]$ sudo ssh root@192.168.123.100
----


. Download the cli called `hcp`
+
[source,bash]
----
[root@ocp4-bastion ~]# curl -O https://hcp-cli-download-multicluster-engine.apps.{guid}.dynamic.redhatworkshops.io/linux/amd64/hcp.tar.gz
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100 32.4M  100 32.4M    0     0   498M      0 --:--:-- --:--:-- --:--:--  498M
----

. Extract the client
+
[source,bash]
----
[root@ocp4-bastion ~]# tar xvfz hcp.tar.gz -C /usr/local/bin/
----

. Confirm the command `hcp` is available 
+
[source,bash]
----
[root@ocp4-bastion ~]# hcp -v
----
+
.Sample Output
+
[%nowrap]
----
hcp version openshift/hypershift: e87182ca75da37c74b371aa0f17aeaa41437561a. Latest supported OCP: 4.14.0
----


[#create]
== Create Hosted Cluster

Some considerations:

* Run the hub cluster and workers on the same platform for hosted control planes.
* Each hosted cluster must have a unique name in order for multicluster engine operator to manage it.
* A hosted cluster cannot be created in the namespace of a multicluster engine operator managed cluster.

TODO: login or generate .kube/config because of the new TLS

. Create a new hosted cluster using the command `hcp`
+
[source,bash]
----
[root@ocp4-bastion ~]# hcp create cluster kubevirt \
--insecure-skip-tls-verify=true \
--name cluster1 \
--node-pool-replicas 2 \
--pull-secret ~/pull-secret.json \
--memory 6Gi \
--cores 2
----
+
.Sample Output
+
[%nowrap]
----
2023-12-02T21:49:55Z    INFO    Applied Kube resource   {"kind": "Namespace", "namespace": "", "name": "clusters"}
2023-12-02T21:49:55Z    INFO    Applied Kube resource   {"kind": "Secret", "namespace": "clusters", "name": "cluster1-pull-secret"}
2023-12-02T21:49:55Z    INFO    Applied Kube resource   {"kind": "", "namespace": "clusters", "name": "cluster1"}
2023-12-02T21:49:55Z    INFO    Applied Kube resource   {"kind": "Secret", "namespace": "clusters", "name": "cluster1-etcd-encryption-key"}
2023-12-02T21:49:55Z    INFO    Applied Kube resource   {"kind": "NodePool", "namespace": "clusters", "name": "cluster1"}
----
+
[NOTE]
You can use the --release-image flag to set up the hosted cluster with a specific OpenShift Container Platform release.
+
A default node pool is created for the cluster with two virtual machine worker replicas according to the `--node-pool-replicas` flag.

. Verify the host control plane pods running inside of the new namespace created (`clusters-cluster1`)
+
[%nowrap]
----
[root@ocp4-bastion ~]# oc get pod -n clusters-cluster1
----
+
.Sample Output
+
[%nowrap]
----
NAME                                      READY   STATUS    RESTARTS   AGE
capi-provider-554c58b965-6cx78            1/1     Running   0          4m23s
cluster-api-5f5b78d889-2tnqm              1/1     Running   0          4m24s
control-plane-operator-67b7d4556b-4b4mq   1/1     Running   0          4m23s
----

. Check the status of the host clusters, querying the _Custom Resource_ named `HostedCluster`.
+
[%nowrap]
----
[root@ocp4-bastion ~]# oc get --namespace clusters hostedclusters
----
+
.Sample Output
+
[%nowrap]
----
----



== Enable Service

The next thing to do is to use a systemctl to enable the service. The ubi-init provides full systemd service to the container.
This will start up any of the services that are enabled. 

. Enable the `httpd` service using `buildah run`

+
[source,bash]
----
buildah run ubi-working-container-1 -- systemctl enable httpd
----

NOTE: The UBI standard image doesn't have systemd installed on it by default. The `ubi-init` container image does include systemd. Plan accordingly to which image you use to build applications.  

== Explore Source Materials

A GitHub repository has already been synchronized to `/home/devops/clumsy-bird/` This repository contains the configurations for your application.

. Verify the repository is cloned inside the `/home/devops/` directory
+
[source,bash]
----
cd /home/devops/
----

+
[source,bash]
----
git clone https://github.com/ellisonleao/clumsy-bird
----
+
NOTE: The output states that the software has already been checked out. 
+
. Explore the JavaScript contents, index files, and everything else that we need for this web-based JavaScript software.
+
[source,bash,role=execute]
----
ls clumsy-bird
----

== Add Source Materials to Container

. Now that you have verified the source materials exist, you will put the software inside of my container image using the `buildah copy` command.
+
[source,bash]
----
buildah copy ubi-working-container-1 clumsy-bird /var/www/html
----

[#container]
== Enable Container in Background

In the previous section of this lab, you ran the game natively through interactive mode. Because you are now building a web application you will need to run the container in the background and access it through a web address. 

. Execute `buildah config` to specify a port and initialize the container. 

+
[source,bash]
----
buildah config --port 80 --cmd "/usr/sbin/init" ubi-working-container-1
----

NOTE: The command makes a configuration change to this container. I'm configuring this container to accept connections to its port 80. When the container starts up, it should run the init command, which, in this case, is going to be the Apache daemon right from earlier when I enabled it.

== Commit Container and Deploy

. Now that we have a container in a working configured state, it is time to make that permanent by committing it to a container image called `clumsy-bird`
+
[source,bash]
----
buildah commit ubi-working-container-1 clumsy-bird
----

. Verify that the image was created with `podman`
+
[source,bash]
----
podman images
----
+
. Run the container Now it's time to run the container. 
+
[source,bash]
----
podman run -d -p 8500:80 clumsy-bird
----

== Verify Application

Verify that the application is running by navigating to

http://bastion.{guid}.example.opentlc.com:8500

