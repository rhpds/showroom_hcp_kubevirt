= Creating a Hosted Cluster on OpenShift Virtualization

You can use the hosted control plane command line interface, `hcp`, to create an *OpenShift Container Platform hosted cluster*. The hosted cluster is automatically imported as a managed cluster.

== Size guidance

See the following highly available hosted control plane requirements, which were tested with OpenShift Container Platform version 4.17.3 and later:

* 78 pods
* Three 8 GiB PVs for etcd
* Minimum vCPU: approximately 5.5 cores
* Minimum memory: approximately 19 GiB

More information: https://access.redhat.com/documentation/en-us/red_hat_advanced_cluster_management_for_kubernetes/2.9/html/clusters/cluster_mce_overview#hosted-sizing-guidance

== Prerequisites

. Connect to the bastion host of your environment
+
[source,bash,role=execute]
----
{bastion_ssh_command}
----


. Verify you login to the OpenShift Cluster (your hosting cluster)
+
[source,bash,role=execute,subs="attributes"]
----
oc version
----
+
.Sample Output
+
[%nowrap]
----
Client Version: 4.17.3
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: 4.17.3
Kubernetes Version: v1.30.5
----
+
[NOTE]
The output above is the OpenShift Cluster version.

// TODO: is this necessary?
. Allow wildcard routes for your Ingress Controllers
+
[source,bash,role=execute,subs="attributes"]
----
oc patch ingresscontroller -n openshift-ingress-operator default --type=json -p '[{ "op": "add", "path": "/spec/routeAdmission", "value": {wildcardPolicy: "WildcardsAllowed"}}]'
----
+
[NOTE]
This command was already executed for you to avoid disconnect your terminal.

== Install hcp CLI

. When Red Hat Advanced Cluster Manager for Kubernes is installed on a hosting cluster it provides a convenient download location for the `hcp` command line tool. Download the cli from your cluster:
+
[source,bash,role=execute,subs="attributes"]
----
curl -O https://hcp-cli-download-multicluster-engine.apps.cluster-{guid}.{guid}.gcp.redhatworkshops.io/linux/amd64/hcp.tar.gz
----

. Extract the client
+
[source,bash,role=execute]
----
tar xvfz hcp.tar.gz -C /usr/local/bin/
----

. Confirm that the command `hcp` is available
+
[source,bash,role=execute]
----
hcp version
----
+
.Sample Output
+
[%nowrap]
----
Client Version: openshift/hypershift: b9e977da802d07591cd9fb8ad91ba24116f4a3a8. Latest supported OCP: 4.17.0
Server Version: b9e977da802d07591cd9fb8ad91ba24116f4a3a8
Server Supports OCP Versions: 4.17, 4.16, 4.15, 4.14
----

[#create]
== Create Hosted Cluster

Some considerations:

* Run the hub cluster and workers on the same platform for hosted control planes.
* Each hosted cluster must have a unique name in order for multicluster engine operator to manage it.
* A hosted cluster cannot be created in the namespace of a multicluster engine operator managed cluster.

. Create a new hosted cluster named _cluster1_ using the command `hcp`:
+
[source,bash,role=execute]
----
hcp create cluster kubevirt \
--name cluster1 \
--release-image quay.io/openshift-release-dev/ocp-release:4.16.21-multi \
--node-pool-replicas 2 \
--pull-secret ~/pull-secret.json \
--memory 6Gi \
--cores 2
----
+
.Sample Output
+
[%nowrap]
----
2024-11-14T20:59:10Z    INFO    Applied Kube resource   {"kind": "Namespace", "namespace": "", "name": "clusters"}
2024-11-14T20:59:10Z    INFO    Applied Kube resource   {"kind": "Secret", "namespace": "clusters", "name": "cluster1-pull-secret
"}
2024-11-14T20:59:11Z    INFO    Applied Kube resource   {"kind": "", "namespace": "clusters", "name": "cluster1"}
2024-11-14T20:59:11Z    INFO    Applied Kube resource   {"kind": "Secret", "namespace": "clusters", "name": "cluster1-etcd-encryp
tion-key"}
2024-11-14T20:59:11Z    INFO    Applied Kube resource   {"kind": "NodePool", "namespace": "clusters", "name": "cluster1"}
----
+
[NOTE]
The parameter `--release-image` flag to set up the hosted cluster with a specific OpenShift Container Platform release.
+
A default node pool is created for the cluster with two virtual machine worker replicas according to the `--node-pool-replicas` flag.

. Verify the host control plane are being created inside of the new namespace created (`clusters-cluster1`)
+
[source,bash,role=execute]
----
oc get pod -n clusters-cluster1
----
+
.Sample Output
+
[%nowrap]
----
NAME                                      READY   STATUS    RESTARTS   AGE
capi-provider-b49b96dc8-nhhpt             1/1     Running   0          45s
cluster-api-5cddb86456-lsfxw              1/1     Running   0          45s
control-plane-operator-5959f6bd5b-qh8k9   1/1     Running   0          44s
----

. Check the status of the host clusters, querying the _Custom Resource_ named `HostedCluster`.
+
[source,bash,role=execute]
----
oc get --namespace clusters hostedclusters
----
+
.Sample Output
+
[%nowrap]
----
NAME       VERSION   KUBECONFIG                  PROGRESS   AVAILABLE   PROGRESSING   MESSAGE
cluster1             cluster1-admin-kubeconfig   Partial    False       False         Waiting for Kube APIServer deployment to become available
----
+
[IMPORTANT]
It takes around 15 minutes until the cluster switches from Partial to Completed. You do not have to wait for the cluster to be fully deployed at this point and you may continue with the lab.

. Go back to the ACM Console (`All Clusters` in the top menu)
. Notice that the `cluster1` will appear automatically. If you don't see it yet, wait until it appears.
+
image::_images/Install/01_ACM_Review.png[]

. Click on `cluster1` and check the progress
+
image::_images/Install/02_ACM_Progress.png[]

. Review the information and scroll down and wait until the status is `Ready`

. In the middle of the screen, press `Control plane pods`
+
image::_images/Install/02_ACM_CP_pods.png[]

. Expect to see a big list of pods, as shown in the following image:
+
image::_images/Install/02_ACM_CP_pods_list.png[]

The _Hosted Control Plane_ and _Data Plane_ use the _Konnectivity_ service to establish a tunnel for communications from the control plane to the data plane. This connection works as follows:

The _Konnectivity_ agent in the compute nodes, connects with the _Konnectivity_ server running as part of the _Hosted Control Plane_.

The Kubernetes API Server uses this tunnel to communicate with the kubelet running on each compute node.

The compute nodes reach the Hosted Cluster API via an exposed service. Depending on the infrastructure where the _Hosted Control Plane_ runs this service can be exposed via a load balancer, a node port, etc.

The _Konnectivity_ server is used by the _Hosted Control Plane_ to consume services deployed in the hosted cluster namespace such as OLM, OpenShift Aggregated API and OAuth.

image::_images/Review/hcp-dp-connection.png[]

== Review creation

. While the installation continues, check OpenShift Virtualization
.. Switch back to your `local-cluster`
.. In the left menu navigate to *Virtualization* -> *Virtual Machines*
.. Select project `clusters-cluster1`
+
image::_images/Install/03_OCPV_VMs.png[]
+
CoreOS disks are imported automatically and the VMs starts. They will act as a workers for the hosted control planes cluster.

[CAUTION]
====
In some situations in the lab we have detected the servers are failing reaching the ignition server. If the installation is taking long and accessing the console of the VM is showing failures accessing to ignition server please remove the _router_ pods.
+
. _Run only if VMS are failing to reach ignition server_
+
[source,bash,role=execute]
----
oc delete pods -n openshift-ingress --all
----
====

. In the left menu navigate to *Networking* -> *Services*:
+
image::_images/Install/04_OCPV_Services.png[]
+
Notice the required services for OpenShift are created inside the namespace for each cluster that has been created.

. Navigate to *Networking* -> *Routes*:
+
image::_images/Install/05_OCPV_Routes.png[]
+
The routes to access the hosted cluster from the internet are listed.

. Navigate to *Storage* -> *PersistentVolumeClaims*
+
image::_images/Install/06_OCPV_PVCs.png[]
+
Notice that the *etcd* disks for the control plane are created. These disks are used for the control planes pods. It is recommended to use a low-latency and fast I/O disks for etcd to avoid issues.

== Review creation

. Go back to *ACM* console and select `cluster1` and wait until the cluster creation is complete.
+
image::_images/Install/07_OCPV_Ready.png[]
+
The cluster can be `Ready` but the worker nodes may still be provisioning - which also means that *Cluster Operators* are still rolling out.
+
Wait until the `Cluster node pools` section switches from `Pending` to `Ready`

. Review the *Details* information of the cluster
+
image::_images/Install/08_OCPV_Guest_Details.png[]

. Click `Reval credentials` and copy the password for the `kubeadmin` user

. Click on the `Console URL` and accept the self-signed certificate

. Login to the new cluster with the credentials
+
image::_images/Install/09_OCPV_Guest_Home.png[]
+
Notice the `Infrastructure provider` is `KubeVirt`.

. Navigate in the left menu to *Compute* -> *Nodes* and review the workers
+
image::_images/Install/10_OCPV_Guest_Nodes.png[]

. Navigate to the left menu to *Storage* -> *StorageClasses*
+
image::_images/Install/11_OCPV_Guest_StorageClass.png[]
+
Storage Class is a _interface_ to the host OpenShift Cluster storage class. Storage will be covered with more detail later on.

== Review the cluster using the CLI

It is possible download the `kubeconfig` using the UI interface and using the command `hcp.`

. Generate the kubeconfig for the `cluster1` cluster
+
[source,bash,role=execute]
----
hcp create kubeconfig --name cluster1 > cluster1-kubeconfig
----

. Check the cluster operators
+
[source,bash,role=execute]
----
oc get co --kubeconfig=cluster1-kubeconfig
----
+
.Sample Output
+
[%nowrap]
----
NAME                                       VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE   MESSAGE
console                                    4.16.21   True        False         False      12m
csi-snapshot-controller                    4.16.21   True        False         False      26m
dns                                        4.16.21   True        False         False      12m
image-registry                             4.16.21   True        False         False      13m
ingress                                    4.16.21   True        False         False      12m
insights                                   4.16.21   True        False         False      15m
kube-apiserver                             4.16.21   True        False         False      26m
kube-controller-manager                    4.16.21   True        False         False      26m
kube-scheduler                             4.16.21   True        False         False      26m
kube-storage-version-migrator              4.16.21   True        False         False      14m
monitoring                                 4.16.21   True        False         False      4m51s
network                                    4.16.21   True        False         False      13m
node-tuning                                4.16.21   True        False         False      17m
openshift-apiserver                        4.16.21   True        False         False      26m
openshift-controller-manager               4.16.21   True        False         False      26m
openshift-samples                          4.16.21   True        False         False      11m
operator-lifecycle-manager                 4.16.21   True        False         False      27m
operator-lifecycle-manager-catalog         4.16.21   True        False         False      27m
operator-lifecycle-manager-packageserver   4.16.21   True        False         False      26m
service-ca                                 4.16.21   True        False         False      14m
storage                                    4.16.21   True        False         False      27m
----

. Check the cluster nodes
+
[source,bash,role=execute]
----
oc get nodes --kubeconfig=cluster1-kubeconfig
----
+
.Sample Output
+
[%nowrap]
----
NAME                      STATUS   ROLES    AGE   VERSION
cluster1-7e351675-fh675   Ready    worker   18m   v1.29.9+5865c5b
cluster1-7e351675-fhc49   Ready    worker   20m   v1.29.9+5865c5b
----